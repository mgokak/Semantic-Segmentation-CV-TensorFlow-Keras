{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbhn2sJc6M17"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install albumentations"
      ],
      "metadata": {
        "id": "ZxmwKUQw6XIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python"
      ],
      "metadata": {
        "id": "hQdJJrIK6Z4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import glob as glob\n",
        "import albumentations as A\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import PyDataset, to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, Input,\n",
        "                                     Conv2DTranspose,\n",
        "                                     Dropout, concatenate, Activation)\n",
        "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter)\n",
        "from dataclasses import dataclass\n",
        "\n",
        "block_plot = False\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()"
      ],
      "metadata": {
        "id": "OHExOnqu6csm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def system_config(SEED_VALUE):\n",
        "\n",
        "    np.random.seed(SEED_VALUE)\n",
        "    tf.random.set_seed(SEED_VALUE)\n",
        "\n",
        "    # Get list of GPUs.\n",
        "    gpu_devices = tf.config.list_physical_devices('GPU')\n",
        "    print(gpu_devices)\n",
        "\n",
        "    if len(gpu_devices) > 0:\n",
        "        print('Using GPU')\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "        os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "\n",
        "        # If there are any gpu devices, use first gpu.\n",
        "        tf.config.experimental.set_visible_devices(gpu_devices[0], 'GPU')\n",
        "\n",
        "        # Grow the memory usage as it is needed by the process.\n",
        "        tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
        "\n",
        "        # Enable using cudNN.\n",
        "        os.environ['TF_USE_CUDNN'] = \"true\"\n",
        "    else:\n",
        "        print('Using CPU')\n",
        "\n",
        "system_config(7)"
      ],
      "metadata": {
        "id": "6LdQfeP16kFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass(frozen=True)\n",
        "class DatasetConfig:\n",
        "    NUM_CLASSES: int = 32\n",
        "    IMG_WIDTH:   int = 256\n",
        "    IMG_HEIGHT:  int = 256\n",
        "    DATA_TRAIN_IMAGES: str = 'dataset_camvid_trainval/train/*.png'\n",
        "    DATA_TRAIN_LABELS: str = 'dataset_camvid_trainval/train_labels/*.png'\n",
        "    DATA_VALID_IMAGES: str = 'dataset_camvid_trainval/val/*.png'\n",
        "    DATA_VALID_LABELS: str = 'dataset_camvid_trainval/val_labels/*.png'\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class TrainingConfig:\n",
        "    BATCH_SIZE:      int = 8\n",
        "    EPOCHS:          int = 201\n",
        "    LEARNING_RATE: float = 0.0001\n",
        "    CHECKPOINT_DIR:  str = 'model_checkpoint/UNet_CamVid'\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class InferenceConfig:\n",
        "    NUM_BATCHES:     int = 3"
      ],
      "metadata": {
        "id": "WjlGMuSy6m5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unet(num_classes, shape):\n",
        "\n",
        "    model_input = Input(shape=shape)\n",
        "\n",
        "    # Encoder_block-1.\n",
        "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(model_input)\n",
        "    c1 = Dropout(0.1)(c1)\n",
        "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
        "    p1 = MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    # Encoder_block-2.\n",
        "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
        "    c2 = Dropout(0.1)(c2)\n",
        "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
        "    p2 = MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "    # Encoder_block-3.\n",
        "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
        "    c3 = Dropout(0.2)(c3)\n",
        "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
        "    p3 = MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "    # Encoder_block-4.\n",
        "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n",
        "    c4 = Dropout(0.2)(c4)\n",
        "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n",
        "    p4 = MaxPooling2D((2, 2))(c4)\n",
        "\n",
        "    # Intermedicate_block.\n",
        "    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)\n",
        "    c5 = Dropout(0.3)(c5)\n",
        "    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)\n",
        "\n",
        "    # Decoder_blcok-1.\n",
        "    u6 = Conv2DTranspose(512, (2, 2), strides=(2, 2),padding = 'same')(c5)\n",
        "    # Lateral connection from Encoder_block-4.\n",
        "    u6 = concatenate([u6,c4])\n",
        "    c6 = Conv2D(512, (3, 3), activation='relu', padding= 'same')(u6)\n",
        "    c6 = Dropout(0.2)(c6)\n",
        "    c6 = Conv2D(512, (3, 3), activation='relu',  padding= 'same')(c6)\n",
        "\n",
        "    # Decoder_block-2.\n",
        "    u7 = Conv2DTranspose(256, (2,2), strides = (2, 2), padding= 'same')(c6)\n",
        "    # Lateral connection from Encoder_block-3.\n",
        "    u7 = concatenate([u7,c3])\n",
        "    c7 = Conv2D(256, (3, 3), activation='relu', padding= 'same')(u7)\n",
        "    c7 = Dropout(0.1)(c7)\n",
        "    c7 = Conv2D(256, (3, 3), activation='relu',  padding= 'same')(c7)\n",
        "\n",
        "    # Decoder_block-3.\n",
        "    u8 = Conv2DTranspose(128, (2,2), strides= (2, 2),padding = 'same')(c7)\n",
        "    # Lateral connection from Encoder_blcok-2.\n",
        "    u8 = concatenate([u8,c2])\n",
        "    c8 = Conv2D(128, (3, 3), activation='relu', padding= 'same')(u8)\n",
        "    c8 = Dropout(0.1)(c8)\n",
        "    c8 = Conv2D(128, (3, 3), activation='relu',   padding= 'same')(c8)\n",
        "\n",
        "    # Decoder_blcok-4.\n",
        "    u9 = Conv2DTranspose(64, (2, 2), strides = (2, 2), padding= 'same')(c8)\n",
        "    # Lateral connection from Encoder_blcok-1.\n",
        "    u9 = concatenate([u9,c1], axis =3)\n",
        "    c9 = Conv2D(64, (3, 3), activation ='relu',  padding='same')(u9)\n",
        "    c9 = Dropout(0.1)(c9)\n",
        "    c9 = Conv2D(64, (3, 3), activation ='relu',  padding='same')(c9)\n",
        "\n",
        "    # 1x1 convolution to limit the depth of the feature maps to the number of classes.\n",
        "    outputs = Conv2D(num_classes, (1, 1), use_bias=False)(c9)\n",
        "\n",
        "    model_output = Activation('softmax')(outputs)\n",
        "\n",
        "    model = Model(inputs=model_input, outputs=model_output)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "t2X7W5lh6pQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = unet(num_classes=DatasetConfig.NUM_CLASSES,\n",
        "             shape=(DatasetConfig.IMG_HEIGHT, DatasetConfig.IMG_WIDTH, 3))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "6EHryPMe6sUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To Download processed dataset.\n",
        "def download_file(url, save_name):\n",
        "    file = requests.get(url)\n",
        "    open(save_name, 'wb').write(file.content)"
      ],
      "metadata": {
        "id": "nlKOZcFn6vAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To Unzip the dataset file.\n",
        "def unzip(zip_file=None):\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_file) as z:\n",
        "            z.extractall(\"./\")\n",
        "            print(\"Extracted all\")\n",
        "    except:\n",
        "        print(\"Invalid file\")"
      ],
      "metadata": {
        "id": "Ya-y37eL6zRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_name = \"dataset_camvid_trainval.zip\"\n",
        "\n",
        "if not os.path.exists(save_name):\n",
        "    download_file(\n",
        "        \"https://www.dropbox.com/s/nkzsucbeblzq3dm/dataset_camvid_trainval.zip?dl=1\",\n",
        "        save_name\n",
        "    )\n",
        "\n",
        "    unzip(zip_file=save_name)"
      ],
      "metadata": {
        "id": "WzL0Fai861Pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Class for creating training and validation (segmentation) dataset objects.\n",
        "class CustomSegDataLoader(PyDataset):\n",
        "\n",
        "    def __init__(self, batch_size, image_size, image_paths, mask_paths, num_classes, aug, **kwargs):\n",
        "\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.batch_size  = batch_size\n",
        "        self.image_size  = image_size\n",
        "        self.image_paths = image_paths\n",
        "        self.mask_paths  = mask_paths\n",
        "        self.num_classes = num_classes\n",
        "        self.aug = aug\n",
        "\n",
        "        self.x = np.empty((self.batch_size,) + self.image_size + (3,), dtype=\"float32\")\n",
        "        self.y = np.empty((self.batch_size,) + self.image_size, dtype=\"float32\")\n",
        "\n",
        "        if self.aug:\n",
        "            self.train_transforms = self.transforms()\n",
        "\n",
        "        self.resize_transforms = self.resize()\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.mask_paths) // self.batch_size\n",
        "\n",
        "    def transforms(self):\n",
        "\n",
        "        # Data augmentation.\n",
        "        train_transforms = A.Compose([\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.ShiftScaleRotate(scale_limit=0.2, rotate_limit=0,\n",
        "                               shift_limit=0.2, p=0.5, border_mode=0),\n",
        "        ])\n",
        "        return train_transforms\n",
        "\n",
        "    def resize(self):\n",
        "\n",
        "        resize_transforms = A.Resize(\n",
        "            height=self.image_size[0], width=self.image_size[1],\n",
        "            interpolation=cv2.INTER_NEAREST,\n",
        "            always_apply=True, p=1\n",
        "        )\n",
        "        return resize_transforms\n",
        "\n",
        "    def reset_array(self):\n",
        "        self.x.fill(0.)\n",
        "        self.y.fill(0.)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        self.reset_array()\n",
        "        i = idx * self.batch_size\n",
        "        batch_image_paths = self.image_paths[i : i + self.batch_size]\n",
        "        batch_mask_paths = self.mask_paths[i : i + self.batch_size]\n",
        "\n",
        "        for j, (input_image, input_mask) in enumerate(zip(batch_image_paths, batch_mask_paths)):\n",
        "\n",
        "            # Read the image and convert to RGB.\n",
        "            img = cv2.imread(input_image)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Read the mask and convert to RGB.\n",
        "            msk = cv2.imread(input_mask)\n",
        "            msk = cv2.cvtColor(msk, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Resize the image and mask.\n",
        "            resized  = self.resize_transforms(image=img, mask=msk)\n",
        "            img, msk = resized['image'], resized['mask']\n",
        "\n",
        "            if self.aug:\n",
        "                # Apply augmentations.\n",
        "                train_augment = self.train_transforms(image=img, mask=msk)\n",
        "                img, msk = train_augment['image'], train_augment['mask']\n",
        "\n",
        "            # Store image in x.\n",
        "            self.x[j] = img / 255. # Normalizing image to be in range [0.0, 1.0]\n",
        "\n",
        "            # Convert RGB segmentation mask to multi-channel (one-hot) encoded arrays where\n",
        "            # each channel represents a single class whose pixel values are either 0 or 1,\n",
        "            # where a 1 represents a pixel location associated with the class that corresponds\n",
        "            # to the channel.\n",
        "            msk = rgb_to_onehot(msk)\n",
        "\n",
        "            # Convert the multi-channel (one-hot encoded) mask to a single channel (grayscale)\n",
        "            # representation whose values contain the class IDs for each class (essentially\n",
        "            # collapsing the one-hot encoded arrays into a single channel).\n",
        "            self.y[j] = msk.argmax(-1)\n",
        "\n",
        "        return self.x, self.y"
      ],
      "metadata": {
        "id": "Yr8JKEmb63J2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary mapping class IDs to colors from the original dataset ground truth masks.\n",
        "id2color = {\n",
        "    0: (64, 128, 64),\n",
        "    1: (192, 0, 128),\n",
        "    2: (0, 128, 192),\n",
        "    3: (0, 128, 64),\n",
        "    4: (128, 0, 0),\n",
        "    5: (64, 0, 128),\n",
        "    6: (64, 0, 192),\n",
        "    7: (192, 128, 64),\n",
        "    8: (192, 192, 128),\n",
        "    9: (64, 64, 128),\n",
        "    10: (128, 0, 192),\n",
        "    11: (192, 0, 64),\n",
        "    12: (128, 128, 64),\n",
        "    13: (192, 0, 192),\n",
        "    14: (128, 64, 64),\n",
        "    15: (64, 192, 128),\n",
        "    16: (64, 64, 0),\n",
        "    17: (128, 64, 128),\n",
        "    18: (128, 128, 192),\n",
        "    19: (0, 0, 192),\n",
        "    20: (192, 128, 128),\n",
        "    21: (128, 128, 128),\n",
        "    22: (64, 128, 192),\n",
        "    23: (0, 0, 64),\n",
        "    24: (0, 64, 64),\n",
        "    25: (192, 64, 128),\n",
        "    26: (128, 128, 0),\n",
        "    27: (192, 128, 192),\n",
        "    28: (64, 0, 64),\n",
        "    29: (192, 192, 0),\n",
        "    30: (0,0, 0),\n",
        "    31: (64, 192, 0)\n",
        " }"
      ],
      "metadata": {
        "id": "GrdKf3w-67jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to one-hot encode RGB mask labels.\n",
        "def rgb_to_onehot(rgb_arr, color_map=id2color, num_classes=DatasetConfig.NUM_CLASSES):\n",
        "\n",
        "    shape = rgb_arr.shape[:2] + (num_classes,)\n",
        "    arr = np.zeros( shape, dtype=np.float32 )\n",
        "\n",
        "    for i, classes in enumerate(color_map):\n",
        "        arr[:,:,i] = np.all(rgb_arr.reshape( (-1,3) ) == color_map[i], axis=1).reshape(shape[:2])\n",
        "\n",
        "    return arr"
      ],
      "metadata": {
        "id": "Ej79Huar6_ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert a single channel mask representation to an RGB mask.\n",
        "def num_to_rgb(num_arr, color_map=id2color):\n",
        "\n",
        "    single_layer = np.squeeze(num_arr)\n",
        "    output = np.zeros(num_arr.shape[:2]+(3,))\n",
        "\n",
        "    for k in color_map.keys():\n",
        "        output[single_layer==k] = color_map[k]\n",
        "\n",
        "    return np.float32(output) / 255. # return a floating point array in range [0.0, 1.0]"
      ],
      "metadata": {
        "id": "U8UjcF6p7CSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to overlay a segmentation map on top of an RGB image.\n",
        "def image_overlay(image, segmented_image):\n",
        "\n",
        "    alpha = 1.0 # Transparency for the original image.\n",
        "    beta  = 0.7 # Transparency for the segmentation map.\n",
        "    gamma = 0.0 # Scalar added to each sum.\n",
        "\n",
        "    segmented_image = cv2.cvtColor(segmented_image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    image = cv2.addWeighted(image, alpha, segmented_image, beta, gamma, image)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    return image"
      ],
      "metadata": {
        "id": "qVZZCunz7JIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_image_and_mask(data_list, color_mask=False, color_map=id2color):\n",
        "\n",
        "    plt.figure(figsize=(16, 6))\n",
        "    title = ['GT Image', 'GT Mask', 'Overlayed Mask']\n",
        "\n",
        "    grayscale_gt_mask = data_list[1]\n",
        "\n",
        "    # Create RGB segmentation map from grayscale segmentation map.\n",
        "    rgb_gt_mask = num_to_rgb(data_list[1], color_map=color_map)\n",
        "\n",
        "    # Create the overlayed image.\n",
        "    overlayed_image = image_overlay(data_list[0], rgb_gt_mask)\n",
        "\n",
        "    data_list.append(overlayed_image.clip(0.0, 1.0))\n",
        "\n",
        "    for i in range(len(data_list)):\n",
        "        plt.subplot(1, len(data_list), i+1)\n",
        "        plt.title(title[i])\n",
        "        if title[i] == 'GT Mask':\n",
        "            if color_mask:\n",
        "                plt.imshow(np.array(rgb_gt_mask))\n",
        "            else:\n",
        "                plt.imshow(np.array(grayscale_gt_mask))\n",
        "        else:\n",
        "            plt.imshow(np.array(data_list[i]))\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "JC4ywwY87MyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_datasets(aug=False):\n",
        "\n",
        "    # Training image and mask paths.\n",
        "    train_images = sorted(glob.glob(f\"{DatasetConfig.DATA_TRAIN_IMAGES}\"))\n",
        "    train_masks  = sorted(glob.glob(f\"{DatasetConfig.DATA_TRAIN_LABELS}\"))\n",
        "\n",
        "    # Validation image and mask paths.\n",
        "    valid_images = sorted(glob.glob(f\"{DatasetConfig.DATA_VALID_IMAGES}\"))\n",
        "    valid_masks  = sorted(glob.glob(f\"{DatasetConfig.DATA_VALID_LABELS}\"))\n",
        "\n",
        "    # Train data loader.\n",
        "    train_ds = CustomSegDataLoader(batch_size=TrainingConfig.BATCH_SIZE,\n",
        "                                   image_size=(DatasetConfig.IMG_HEIGHT, DatasetConfig.IMG_WIDTH),\n",
        "                                   image_paths=train_images,\n",
        "                                   mask_paths=train_masks,\n",
        "                                   num_classes=DatasetConfig.NUM_CLASSES,\n",
        "                                   aug=aug,\n",
        "                                   workers = 4,\n",
        "                                   use_multiprocessing = True\n",
        "                                  )\n",
        "    # Validation data loader.\n",
        "    valid_ds = CustomSegDataLoader(batch_size=TrainingConfig.BATCH_SIZE,\n",
        "                                   image_size=(DatasetConfig.IMG_HEIGHT, DatasetConfig.IMG_WIDTH),\n",
        "                                   image_paths=valid_images,\n",
        "                                   mask_paths=valid_masks,\n",
        "                                   num_classes=DatasetConfig.NUM_CLASSES,\n",
        "                                   aug=False,\n",
        "                                   workers = 4,\n",
        "                                   use_multiprocessing = True\n",
        "                                  )\n",
        "\n",
        "    return train_ds, valid_ds"
      ],
      "metadata": {
        "id": "5Pk6wN1R7QIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds, valid_ds = create_datasets(aug=True)"
      ],
      "metadata": {
        "id": "fEJzGNci7TbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (images, masks) in enumerate(train_ds):\n",
        "    if i == 3:\n",
        "        break\n",
        "    image, mask = images[0], masks[0]\n",
        "    display_image_and_mask([image, mask], color_mask=True)"
      ],
      "metadata": {
        "id": "YS4MZ1LT7d1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_iou(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    y_true (ndarray or tensor): Ground truth mask (G). Shape: (batch_size, height, width)\n",
        "                               Sparse representation of segmentation mask.\n",
        "\n",
        "    y_pred (ndarray or tensor): Prediction (P) from the model with or without softmax.\n",
        "                                Shape: (batch_size, height, width, num_classes).\n",
        "\n",
        "    return (scalar): Classwise mean IoU Metric.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get total number of classes from model output.\n",
        "    num_classes = y_pred.shape[-1]\n",
        "\n",
        "    # Convert single channel (sparse) ground truth labels to one-hot encoding for metric computation.\n",
        "    y_true = tf.one_hot(tf.cast(y_true, tf.int32), num_classes, axis=-1)\n",
        "\n",
        "    # Convert multi-channel predicted output to one-hot encoded thresholded output for metric computation.\n",
        "    y_pred = tf.one_hot(tf.math.argmax(y_pred, axis=-1), num_classes, axis=-1)\n",
        "\n",
        "    # Axes corresponding to image width and height: [B, H, W, C].\n",
        "    axes = (1, 2)\n",
        "\n",
        "    # Intersection: |G ∩ P|. Shape: (batch_size, num_classes)\n",
        "    intersection = tf.math.reduce_sum(y_true * y_pred, axis=axes)\n",
        "\n",
        "    # Total Sum: |G| + |P|. Shape: (batch_size, num_classes)\n",
        "    total = tf.math.reduce_sum(y_true, axis=axes) + tf.math.reduce_sum(y_pred, axis=axes)\n",
        "\n",
        "    # Union: Shape: (batch_size, num_classes)\n",
        "    union = total - intersection\n",
        "\n",
        "    # Boolean (then converted to float) value for each class if it is present or not.\n",
        "    # Shape: (batch_size, num_classes)\n",
        "    is_class_present =  tf.cast(tf.math.not_equal(total, 0), dtype=tf.float32)\n",
        "\n",
        "    # Sum along axis(1) to get number of classes in each image.\n",
        "    # Shape: (batch_size,)\n",
        "    num_classes_present = tf.math.reduce_sum(is_class_present, axis=1)\n",
        "\n",
        "    # Here, we use tf.math.divide_no_nan() to prevent division by 0 (i.e., 0/0 = 0).\n",
        "    # Shape: (batch_size, num_classes)\n",
        "    iou = tf.math.divide_no_nan(intersection, union)\n",
        "\n",
        "    # IoU per image. Average over the total number of classes present in y_true and y_pred.\n",
        "    # Shape: (batch_size,)\n",
        "    iou = tf.math.reduce_sum(iou, axis=1) / num_classes_present\n",
        "\n",
        "    # Compute the mean across the batch axis. Shape: Scalar\n",
        "    mean_iou = tf.math.reduce_mean(iou)\n",
        "\n",
        "    return mean_iou"
      ],
      "metadata": {
        "id": "3oMje08S7f00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_coef_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    y_true (ndarray or tensor): Ground truth mask (G). Shape: (batch_size, height, width)\n",
        "                                Sparse representation of segmentation mask.\n",
        "\n",
        "    y_pred (ndarray or tensor): Softmax prediction (P) from the model.\n",
        "                                Shape: (batch_size, height, width, num_classes).\n",
        "\n",
        "    return (scalar): Loss\n",
        "    \"\"\"\n",
        "    num_classes = y_pred.shape[-1]\n",
        "\n",
        "    # Convert single channel ground truth labels to one-hot encoding for metric computation.\n",
        "    y_true = tf.one_hot(tf.cast(y_true, tf.int32), num_classes, axis=-1)\n",
        "\n",
        "    # Axes corresponding to image width and height: [B, H, W, C].\n",
        "    axes = (1, 2)\n",
        "\n",
        "    # Intersection: |G ∩ P|. Shape: (batch_size, num_classes)\n",
        "    intersection = tf.math.reduce_sum(y_true * y_pred, axis=axes)\n",
        "\n",
        "    # Total Sum: |G| + |P|. Shape: (batch_size, num_classes)\n",
        "    total = tf.math.reduce_sum(y_true, axis=axes) + tf.math.reduce_sum(y_pred, axis=axes)\n",
        "\n",
        "    # Dice coefficient. Shape: (batch_size, num_classes)\n",
        "    eps = 1e-3\n",
        "    dc = (2. *  intersection + eps) / (total + eps)\n",
        "\n",
        "    # Compute the mean over the remaining axes (batch and classes).\n",
        "    dc_mean = tf.math.reduce_mean(dc)\n",
        "\n",
        "    # Compute cross-entropy loss.\n",
        "    CCE =  tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
        "\n",
        "    return 1.0 - dc_mean + CCE"
      ],
      "metadata": {
        "id": "uHyhBZht7k0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=Adam(learning_rate=TrainingConfig.LEARNING_RATE),\n",
        "              loss=dice_coef_loss,\n",
        "              metrics=['accuracy', mean_iou],\n",
        "             )"
      ],
      "metadata": {
        "id": "UCIDXfnG7n5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new checkpoint directory every time.\n",
        "if not os.path.exists(TrainingConfig.CHECKPOINT_DIR):\n",
        "    os.makedirs(TrainingConfig.CHECKPOINT_DIR)\n",
        "\n",
        "num_versions = len(os.listdir(TrainingConfig.CHECKPOINT_DIR)) + 1\n",
        "version_dir = TrainingConfig.CHECKPOINT_DIR +'/version_' + str(num_versions)\n",
        "os.makedirs(version_dir)\n",
        "\n",
        "# Callback to save the best model based on validation loss.\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=f\"{version_dir}.keras\",\n",
        "                                                               save_weights_only=False,\n",
        "                                                               monitor='val_mean_iou',\n",
        "                                                               mode='max',\n",
        "                                                               save_best_only=True,\n",
        "                                                              )"
      ],
      "metadata": {
        "id": "O8dchP3M7qMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_ds,\n",
        "                    epochs=TrainingConfig.EPOCHS,\n",
        "                    verbose=1,\n",
        "                    validation_data=valid_ds,\n",
        "                    callbacks=[model_checkpoint_callback],\n",
        "                   )"
      ],
      "metadata": {
        "id": "sM_-5tbi7skG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_results(metrics, ylabel=None, ylim=None, metric_name=None, color=None):\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(15, 5))\n",
        "\n",
        "    if not (isinstance(metric_name, list) or isinstance(metric_name, tuple)):\n",
        "        metrics = [metrics,]\n",
        "        metric_name = [metric_name,]\n",
        "\n",
        "    for idx, metric in enumerate(metrics):\n",
        "        ax.plot(metric, color=color[idx])\n",
        "\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.title(ylabel)\n",
        "    plt.xlim([0, TrainingConfig.EPOCHS-1])\n",
        "    plt.ylim(ylim)\n",
        "    # Tailor x-axis tick marks\n",
        "    ax.xaxis.set_major_locator(MultipleLocator(5))\n",
        "    ax.xaxis.set_major_formatter(FormatStrFormatter('%d'))\n",
        "    ax.xaxis.set_minor_locator(MultipleLocator(1))\n",
        "    plt.grid(True)\n",
        "    plt.legend(metric_name)\n",
        "    plt.show(block=block_plot)\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "5lJxwYUP7uu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pixel accuracy\n",
        "train_acc = history.history[\"accuracy\"]\n",
        "valid_acc = history.history[\"val_accuracy\"]\n",
        "\n",
        "# Mean IoU.\n",
        "train_iou = history.history[\"mean_iou\"]\n",
        "valid_iou = history.history[\"val_mean_iou\"]\n",
        "\n",
        "plot_results([ train_acc, valid_acc ],\n",
        "            ylabel=\"Accuracy\",\n",
        "            ylim = [0.5, 1.0],\n",
        "            metric_name=[\"Training Accuracy\", \"Validation Accuracy\"],\n",
        "            color=[\"g\", \"b\"])\n",
        "\n",
        "plot_results([ train_iou, valid_iou ],\n",
        "            ylabel=\"Mean IoU\",\n",
        "            ylim = [0.0, 1.0],\n",
        "            metric_name=[\"Training IoU\", \"Validation IoU\"],\n",
        "            color=[\"g\", \"b\"])"
      ],
      "metadata": {
        "id": "infoNExn7y9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = history.history[\"loss\"]\n",
        "valid_loss = history.history[\"val_loss\"]\n",
        "\n",
        "max_loss = max(max(train_loss), max(valid_loss))\n",
        "\n",
        "plot_results([ train_loss, valid_loss ],\n",
        "            ylabel='Dice Loss',\n",
        "            ylim = [0.0, max_loss],\n",
        "            metric_name=[\"Training Loss\", \"Validation Loss\"],\n",
        "            color=[\"g\", \"b\"]);"
      ],
      "metadata": {
        "id": "LKkNsPiZ71U0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = tf.keras.models.load_model(f\"{version_dir}.keras\", custom_objects={'mean_iou':mean_iou,\n",
        "                                                                        'dice_coef_loss':dice_coef_loss})"
      ],
      "metadata": {
        "id": "rLWdsekm73z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate = trained_model.evaluate(valid_ds)\n",
        "\n",
        "print(f\"Model evaluation accuracy:   {evaluate[1]*100.:.3f}\")\n",
        "print(f\"Model evaluation IoU:        {evaluate[2]*100.:.3f}\")"
      ],
      "metadata": {
        "id": "6Y9hrdMj753z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model, dataset):\n",
        "\n",
        "    num_batches_to_process = InferenceConfig.NUM_BATCHES\n",
        "\n",
        "    for idx, data in enumerate(dataset):\n",
        "\n",
        "        batch_img, batch_mask = data[0], data[1]\n",
        "        pred_all = (model.predict(batch_img)).astype('float32')\n",
        "        pred_all = pred_all.argmax(-1)\n",
        "        batch_img = (batch_img * 255).astype('uint8')\n",
        "        if idx == num_batches_to_process:\n",
        "            break\n",
        "\n",
        "        for i in range(0, len(batch_img)):\n",
        "\n",
        "            fig = plt.figure(figsize=(20,8))\n",
        "\n",
        "            # Display the original image.\n",
        "            ax1 = fig.add_subplot(1,4,1)\n",
        "            ax1.imshow(batch_img[i])\n",
        "            ax1.title.set_text('Actual frame')\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Display the ground truth mask.\n",
        "            true_mask_rgb = num_to_rgb(batch_mask[i], color_map=id2color)\n",
        "            ax2 = fig.add_subplot(1,4,2)\n",
        "            ax2.set_title('Ground truth labels')\n",
        "            ax2.imshow(true_mask_rgb)\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Display the predicted segmentation mask.\n",
        "            pred_mask_rgb = num_to_rgb(pred_all[i], color_map=id2color)\n",
        "            ax3 = fig.add_subplot(1,4,3)\n",
        "            ax3.set_title('Predicted labels')\n",
        "            ax3.imshow(pred_mask_rgb)\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Display the predicted segmentation mask overlayed on the original image.\n",
        "            overlayed_image = image_overlay(batch_img[i], np.array((pred_mask_rgb)* 255.astype('uint8')))\n",
        "            ax4 = fig.add_subplot(1,4,4)\n",
        "            ax4.set_title('Overlayed image')\n",
        "            ax4.imshow(overlayed_image)\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.show()"
      ],
      "metadata": {
        "id": "yGAeaiZt77c-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, valid_ds = create_datasets()\n",
        "\n",
        "inference(trained_model, valid_ds)"
      ],
      "metadata": {
        "id": "nEp5zH3U7-hZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dir_inference(image_dir, mask_dir, trained_model, num_images_to_process=3):\n",
        "\n",
        "    all_images = sorted(glob.glob(f\"{image_dir}\"))\n",
        "    all_masks  = sorted(glob.glob(f\"{mask_dir}\"))\n",
        "\n",
        "    num_frames_to_skip = 4\n",
        "\n",
        "    num_images_to_process = min(num_images_to_process, len(all_images)/num_frames_to_skip)\n",
        "\n",
        "    i = 0\n",
        "    for k in range(num_images_to_process):\n",
        "\n",
        "        i = i + num_frames_to_skip\n",
        "\n",
        "        plt.figure(figsize=(18, 18))\n",
        "\n",
        "        image = cv2.imread(all_images[i])\n",
        "        mask  = cv2.imread(all_masks[i])\n",
        "\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        mask  = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        orig_image = image.copy()\n",
        "        h, w, _ = orig_image.shape\n",
        "\n",
        "        image = cv2.resize(image, (DatasetConfig.IMG_WIDTH, DatasetConfig.IMG_HEIGHT))\n",
        "        image = tf.convert_to_tensor(image, dtype=tf.float32)\n",
        "        image = tf.expand_dims(image, axis=0)\n",
        "\n",
        "        outputs = trained_model.predict(image)\n",
        "\n",
        "        outputs = tf.squeeze(outputs, axis=0)\n",
        "        outputs = np.array(outputs, dtype=np.float32)\n",
        "        outputs_resized = cv2.resize(outputs, (w, h))\n",
        "        rgb_outputs_resized = num_to_rgb(outputs_resized.argmax(-1), color_map=id2color)\n",
        "        overlayed_image = image_overlay(orig_image, np.array((rgb_outputs_resized * 255).astype('uint8')))\n",
        "\n",
        "        plot_data = [orig_image, mask, rgb_outputs_resized]\n",
        "        plt.imshow(overlayed_image)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        plt.figure(figsize=(16, 8))\n",
        "\n",
        "        for j in range(3):\n",
        "            plt.subplot(1, 3, j+1)\n",
        "            plt.imshow(plot_data[j])\n",
        "            plt.axis('off')\n",
        "            # Show the high resolution overlayed image only.\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "sxo53ATc8AoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir_inference(DatasetConfig.DATA_VALID_IMAGES,\n",
        "              DatasetConfig.DATA_VALID_LABELS,\n",
        "              trained_model,\n",
        "              num_images_to_process=12,\n",
        "             )"
      ],
      "metadata": {
        "id": "uKXwsbQW8Fni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VAUCY6SG8IIR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}